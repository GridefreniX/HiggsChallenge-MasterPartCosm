{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación de los datos del entrenamiento\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "El <strong>tratamiento de los datos</strong>, previo al entrenamiento de la red, es una parte muy importante en este tipo de análisis. Las maneras de representar esa información son muy variadas y es necesario definir una representación que permita extraer la información fácilmente.\n",
    "\n",
    "\n",
    "\n",
    "## 1. Estructuras de datos\n",
    "\n",
    "En este primer apartado se describe como es, desde el punto de vista de la programación, la estructura en vectores que va a entrenar la red neuronal.\n",
    "\n",
    "También se describe como son los datos que se dan para entrenar la red neuronal: el formato en el que vienen dados (fichero csv), las variables disponibles para describir los eventos... etc.\n",
    "\n",
    "### 1.1 Formato de muestras para el entrenamiento de la red neuronal\n",
    "\n",
    "La red se va a programar utilizando el entorno [Keras](https://keras.io/), una librería de Python que permite implementar algoritmos de Deep Learning de manera relativamente simple. Este entorno se describirá en el siguiente Notebook.\n",
    "\n",
    "Keras está programado para trabajar con el paquete [NumPY](http://www.numpy.org/), que introduce en Python estructuras vectoriales e incluye gran variedad de métodos para trabajar con ellas. Los vectores que van a representar a los eventos van a definirse con este paquete. Estos vectores se declaran como objetos ```numpy.array``` y cada elemento que contienen se corresponde con un elemento del vector:\n",
    "\n",
    "```\n",
    "un_vector = numpy.array([elemento1, elemento2, ..., elementoN])\n",
    "```\n",
    "\n",
    "Trabajando con Keras, cada evento ( vector $\\vec{x}^{(i)}$ ) se va a representar con un unico objeto ```numpy.array``` de tal manera que cada elemento del vector se corresponda con el valor que toma cada variable para ese evento:\n",
    "\n",
    "```\n",
    "evento = numpy.array([variable1, variable2, ..., variableN])\n",
    "```\n",
    "\n",
    "Por ejemplo, suponiendo que solo se contase con 3 variables por evento para entrenar la red: el momento transverso del primer lepton, el número de jets y la MET. Se cuenta con un evento donde esas variables toman valores de 33.3 GeV, 2 y 147.0 GeV respectivamente. Entonces ese evento podría representarse para ser leído por la red como:\n",
    "\n",
    "```\n",
    "evento = numpy.array([33.3, 2.0, 147.0])\n",
    "```\n",
    "\n",
    "En la práctica se verá que para que la red trabaje de manera más óptima hay ciertos tratamientos previos que se les pueden aplicar a los valores de las variables (como la normalización) para facilitar el proceso de aprendizaje. \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "### 1.2 Raw Data\n",
    "\n",
    "Los datos se dan en un fichero .csv con toda la información necesaria para el entrenamiento de la red neuronal. Exceptuando el heading, <strong>cada fila se corresponde con un evento de la muestra</strong>.\n",
    "\n",
    "La estructura de vectores ```numpy.array``` descrita en el apartado 1.1 debe crearse a partir de la información guardada en este csv.\n",
    "\n",
    "El fichero csv tiene un total de 33 columnas, que contienen toda la información para cada uno de los eventos disponibles. Tres de las columnas son información acerca del evento que no aportan información física acerca de la colisión, y por lo tanto no se utilizan para entrenar la red. Estas variables son:\n",
    "\n",
    "\n",
    "-```EventId```: Número entero utilizado como número identificador del evento.\n",
    "\n",
    "-```Weight```: Peso para calcular la significancia de señal una vez entrenada la red (que no se utiliza en esta práctica).\n",
    "\n",
    "-```Label```: La etiqueta del evento, en formato ```string```, que indica si el evento es un evento de señal (```'s'```) o un evento de fondo (```'b'```). No se incluye en las muestras de los eventos que van a entrenar la red pero se utiliza para indicarle la clase a la que pertenece un evento y así ejecutar el entrenamiento.\n",
    "\n",
    "Las otras 30 variables son variables físicas que describen la colisión y caracterizan el evento. Son las variables que entrenan la red neuronal:\n",
    "\n",
    "-```DER_mass_MMC```\n",
    "\n",
    "-```DER_mass_transverse_met_lep```\n",
    "\n",
    "-```DER_mass_vis```\n",
    "\n",
    "-```DER_pt_h```\n",
    "\n",
    "-```DER_deltaeta_jet_jet```\n",
    "\n",
    "-```DER_mass_jet_jet```\n",
    "\n",
    "-```DER_prodeta_jet_jet```\n",
    "\n",
    "-```DER_deltar_tau_jet```\n",
    "\n",
    "-```DER_pt_tot```\n",
    "\n",
    "-```DER_sum_pt```\n",
    "\n",
    "-```DER_pt_ratio_lep_tau```\n",
    "\n",
    "-```DER_met_phi_centrality```\n",
    "\n",
    "-```DER_lep_eta_centrality```\n",
    "\n",
    "-```PRI_tau_pt```\n",
    "\n",
    "-```PRI_tau_eta```\n",
    "\n",
    "-```PRI_tau_phi```\n",
    "\n",
    "-```PRI_lep_pt```\n",
    "\n",
    "-```PRI_lep_eta```\n",
    "\n",
    "-```PRI_lep_phi```\n",
    "\n",
    "-```PRI_met```\n",
    "\n",
    "-```PRI_met_phi```\n",
    "\n",
    "-```PRI_met_sumet```\n",
    "\n",
    "-```PRI_jet_num```\n",
    "\n",
    "-```PRI_jet_leading_pt```\n",
    "\n",
    "-```PRI_jet_leading_eta```\n",
    "\n",
    "-```PRI_jet_leading_phi```\n",
    "\n",
    "-```PRI_jet_subleading_pt```\n",
    "\n",
    "-```PRI_jet_subleading_eta```\n",
    "\n",
    "-```PRI_jet_subleading_phi```\n",
    "\n",
    "-```PRI_jet_all_pt```\n",
    "\n",
    "\n",
    "\n",
    "En algunos eventos es posible que una o algunas de las variables no existan o no tengan significado físico i.e. la variable```PRI_jet_subleading_pt``` (el momento transverso del segundo jet más energético en la colisión) cuando hay menos de 2 jets (```PRI_jet_num``` < 2). En estos casos la variable toma un valor estándar de -999.0. \n",
    "\n",
    "\n",
    "Los eventos deben ser leídos por la red neuronal con el mismo formato (vectores). La red neuronal establece las correlaciones entre variables en función de la posición que ocupan en el vector y si se suprimen directamente los valores que no tienen significado entonces esa estructura desaparece. Por lo tanto, los valores perdidos no se pueden suprimir directamente del vector. Es decir, no es posible que la red lea un vector de 30 elementos y después uno de 29 porque hay uno que está perdido. En cambio, se le debe asignar a todos las entradas un valor único (como -999.0) que no figure en el rango del resto de variables que si están presentes y si tienen sentido físico. Se espera entonces, de esta manera, que la red aprenda por si sola a interpretar que ese valor no es relevante.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lectura de datos en formato .csv\n",
    "\n",
    "El paquete que se utiliza para leer los datos es [pandas](https://pandas.pydata.org/), una librería de Python con métodos para el análisis de datos.\n",
    "\n",
    "Con este paquete se puede leer el csv <em>training.csv</em> y cargar la información en un objeto ```pandas.DataFrame``` ( variable ```data```) al que podemos acceder para ver la clase de información con la que estamos tratando.\n",
    "\n",
    "Se miran las variables de los datos en el heading del csv y el número de eventos. De todas las variables del heading, se tienen que identificar las que van a servir para entrenar la red ( variable ```features```). Estas son todas menos ```'EventId'```, ```'Weight'``` y ```'Label'```. \n",
    "\n",
    "<p> Es muy importante saber cuantos eventos de cada clase hay en la muestra. En este caso, se tienen dos clases: </p>\n",
    "<ul> \n",
    "    <li> <strong>Señal</strong>: Eventos con variable 'Label' = 's' </li>\n",
    "    <li> <strong>Fondo</strong>: Eventos con variable 'Label' = 'b' </li>\n",
    "</ul>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Heading:\n",
      "['EventId', 'DER_mass_MMC', 'DER_mass_transverse_met_lep', 'DER_mass_vis', 'DER_pt_h', 'DER_deltaeta_jet_jet', 'DER_mass_jet_jet', 'DER_prodeta_jet_jet', 'DER_deltar_tau_lep', 'DER_pt_tot', 'DER_sum_pt', 'DER_pt_ratio_lep_tau', 'DER_met_phi_centrality', 'DER_lep_eta_centrality', 'PRI_tau_pt', 'PRI_tau_eta', 'PRI_tau_phi', 'PRI_lep_pt', 'PRI_lep_eta', 'PRI_lep_phi', 'PRI_met', 'PRI_met_phi', 'PRI_met_sumet', 'PRI_jet_num', 'PRI_jet_leading_pt', 'PRI_jet_leading_eta', 'PRI_jet_leading_phi', 'PRI_jet_subleading_pt', 'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_all_pt', 'Weight', 'Label']\n",
      "\n",
      "\n",
      ">>> Number of events: 250000\n",
      "\n",
      "\n",
      "['DER_mass_MMC', 'DER_mass_transverse_met_lep', 'DER_mass_vis', 'DER_pt_h', 'DER_deltaeta_jet_jet', 'DER_mass_jet_jet', 'DER_prodeta_jet_jet', 'DER_deltar_tau_lep', 'DER_pt_tot', 'DER_sum_pt', 'DER_pt_ratio_lep_tau', 'DER_met_phi_centrality', 'DER_lep_eta_centrality', 'PRI_tau_pt', 'PRI_tau_eta', 'PRI_tau_phi', 'PRI_lep_pt', 'PRI_lep_eta', 'PRI_lep_phi', 'PRI_met', 'PRI_met_phi', 'PRI_met_sumet', 'PRI_jet_num', 'PRI_jet_leading_pt', 'PRI_jet_leading_eta', 'PRI_jet_leading_phi', 'PRI_jet_subleading_pt', 'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_all_pt']\n",
      ">>> Number of events in features: 30\n",
      "\n",
      "\n",
      ">>> Number of signal events: 85667\n",
      ">>> Number of background events: 164333\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "data = pandas.read_csv(\"training.csv\", delimiter = ',') # DataFrame object with al the info of the csv\n",
    "\n",
    "# Get the basic information:\n",
    "print(\">>> Heading:\")\n",
    "print(list(data)) # Features and other useful info\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\">>> Number of events: \" + str(len(data)))\n",
    "print(\"\\n\")\n",
    "\n",
    "features = [h for h in list(data) if h not in ['EventId', 'Weight', 'Label']] # select the training features\n",
    "\n",
    "print(features)\n",
    "print(\">>> Number of features: \" + str(len(features)))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Ocurrence of each class\n",
    "n_signal = len(data.loc[data['Label'] == 's'])\n",
    "n_background = len(data.loc[data['Label'] == 'b'])\n",
    "print(\">>> Number of signal events: \" + str(n_signal))\n",
    "print(\">>> Number of background events: \" + str(n_background))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Representación de las variables\n",
    "\n",
    "Se estudia la distribución de las variables de entrenamiento. Se importan los paquetes [NumPy](http://www.numpy.org/) para hacer uso de objetos ```numpy.array``` y [Matplotlib](https://matplotlib.org/) para generar los histogramas.\n",
    "\n",
    "Se crea un histograma con la distribución de cada variable por separado para señal y fondo. Se excluyen de la representación los valores -999.0 para poder estudiar las distribuciones más fácilmente. Cada histograma se guarda con el nombre de la variable en la carpeta Histograms/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "### Loop over the features\n",
    "for feature in features:\n",
    "    \n",
    "    print(\">>> Plotting \" + feature + \" histogram...\")\n",
    "    \n",
    "    # Signal and background values\n",
    "    signal_values = list( data.loc[data['Label'] == 's', feature] )\n",
    "    background_values = list( data.loc[data['Label'] == 'b', feature] )\n",
    "    \n",
    "    signal_values = list(filter(lambda x: x != -999.0, signal_values))\n",
    "    background_values = list(filter(lambda x: x != -999.0, background_values))\n",
    "    \n",
    "    # Define the histogram binning\n",
    "    xmin = min(signal_values + background_values) \n",
    "    xmax = max(signal_values + background_values)\n",
    "    binning = np.linspace(xmin, xmax, 61) \n",
    "    \n",
    "    # Plot and save the histogram\n",
    "    plt.clf()\n",
    "    plt.hist(signal_values, bins = binning, color = 'b', alpha = 0.3, histtype = 'stepfilled', \n",
    "             linewidth = 1, edgecolor = 'b', label = 'Signal')\n",
    "    plt.hist(background_values, bins = binning, color = 'r', alpha = 0.3, histtype = 'stepfilled', \n",
    "             linewidth = 1, edgecolor = 'r', label = 'Background')\n",
    "    plt.legend(loc = 'upper right')\n",
    "    plt.xlabel(feature, fontsize = 12)\n",
    "    plt.ylabel('Entries', fontsize = 12) \n",
    "    plt.savefig('Histograms/'+feature+'_histo.png', dpi = 600)\n",
    "          \n",
    "    print(\"    Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creación de la estructura en vectores \n",
    "\n",
    "En este apartado se explica paso a paso como crear las muestras:\n",
    "\n",
    "4.1. Estructura en vectores. Se introduce la estructura en vectores final a la cual se quiere llegar.\n",
    "\n",
    "4.2. Normalización. Se explica como se deben normalizar los valores de las variables para que el entrenamiento sea más eficiente.\n",
    "\n",
    "4.3. La estructura de las muestras. Se juntan los dos conceptos anteriores, estructura y normalización, para explicar como crear las muestras. Se crean dos estructuras, una para los eventos de fondo y otra para los eventos de señal.\n",
    "\n",
    "4.4. Definición de los sets de entrenamiento: train set y test set. \n",
    "\n",
    "4.5. El balance de clases.\n",
    "\n",
    "4.6. Construcción final de los sets.\n",
    "\n",
    "4.7. Guardar las variables del entrenamiento.\n",
    "\n",
    "### 4.1 Estructura en vectores\n",
    "\n",
    "Anteriormente se introdujo la representación de las muestras de eventos como vectores. También, en el apartado 1.1 se explicó como implementar los vectores en Python como objetos ```numpy.array```. En este apartado se explica como se deben de estrutucturar los vectores de cara al entrenamiento.\n",
    "\n",
    "Para leer los eventos, la red neuronal recibe como input una matriz $\\vec{X}$ con todos los vectores. Después, ella los va leyendo dependiendo de como se haya configurado el proceso de entrenamiento (siguiente Notebook). Esta matriz tiene como filas los vectores construidos a partir de los eventos:\n",
    "\n",
    "$$\\vec{X} = \n",
    "\\begin{pmatrix}\n",
    "\\vec{x}^{(1)} \\\\\n",
    "\\vec{x}^{(2)} \\\\\n",
    "... \\\\\n",
    "\\vec{x}^{(\\text{nEventos})}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Las columnas de esta matriz serán, por lo tanto, los valores de cada una de las variables $j$, cada columna equivale a una variable:\n",
    "\n",
    "$$\\vec{X} = \n",
    "\\begin{pmatrix}\n",
    "\\vec{x}^{(1)}_1 & \\vec{x}^{(1)}_2 & ... & \\vec{x}^{(1)}_{\\text{nVariables}} \\\\\n",
    "\\vec{x}^{(2)}_1 & \\vec{x}^{(2)}_2 & ... & \\vec{x}^{(2)}_{\\text{nVariables}} \\\\\n",
    "... & ... & ...& ... \\\\\n",
    "\\vec{x}^{(\\text{nEventos})}_1 & \\vec{x}^{(\\text{nEventos})}_2 & ... &\\vec{x}^{(\\text{nEventos})}_{\\text{nVariables}} \\\\\n",
    "\\end{pmatrix}$$ \n",
    "\n",
    "\n",
    "Para <strong>implementar una matriz con NumPy</strong> se define un objeto ```numpy.array``` cuyos elementos son a su vez objetos ```numpy.array```. Un ejemplo de creación de una matriz puede ser:\n",
    "\n",
    "```\n",
    "matriz = numpy.array([[1, 2, 3], [4, 5, 6]])\n",
    "```\n",
    "que crearía una matriz de elementos:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Para acceder a un elemento de una matriz en Python se deben especificar la fila ```a``` y la columna ```b``` mediante corchetes, en ese orden (y teniendo en cuenta que los índices empiezan a contar desde 0). Por ejemplo, si quisiesemos cambiar el elemento 5 de la matriz anterior por un 18, habría que hacer:\n",
    "\n",
    "```\n",
    "matriz[1][1] = 8\n",
    "```\n",
    "\n",
    "Tal y como se explica en el apartado 4.3 la creación de la matriz de muestras se va a hacer elemento a elemento utilizando la sintaxis anterior.\n",
    "\n",
    "### 4.2 Normalización\n",
    "\n",
    "Los valores de las variables conviene que se encuentre en el mismo rango, e idealmente tomando valores pequeños. Para ello, los datos se normalizan <strong> variable por variable </strong>. \n",
    "\n",
    "<p> En este caso, los datos se normalizan de tal manera que: </p>\n",
    "<ul>\n",
    "    <li> Su valor central es 0 </li>\n",
    "    <li> Su desviación estándar es igual a 1 </li>\n",
    "</ul>\n",
    "\n",
    "<p> La normalización se compone de dos pasos: </p>\n",
    "<ol>\n",
    "    <li> Para centrar la muestra en 0, se cogen los valores de esa variable de todas las muestras (columna del csv) y se calcula la media. Dicha media se le resta a cada valor. </li>\n",
    "    <li> Para hacer que la distribución de valores tenga valor 1, se calcula la desviación estándar de la columna de valores de la variable y se divide cada uno por dicha desviación estandar. </li>\n",
    "</ol>\n",
    "\n",
    "De acuerdo a la estructura en vectores que va a leer la red, normalizar la muestra variable a variable se correspondería a aplicarle a cada elemento:\n",
    "\n",
    "$$ x'^{(i)}_j = \\dfrac{x^{(i)}_j - \\bar{x}_j}{\\sigma (x_j)}$$\n",
    "\n",
    "Donde $\\bar{x}_j$ es la media de la columna de valores de la variable $j$: \n",
    "$$\\bar{x}_j = \\dfrac{1}{N}\\sum_{i = 1}^{N} x^{(i)}_j$$\n",
    "\n",
    "Y $\\sigma (x_j)$ la desviación estándar de los valores de la columna $j$:\n",
    "\n",
    "$$\\sigma(x_j) = \\sqrt{\\dfrac{\\sum_{i = 1}^{N} (x^{(i)}_j - \\bar{x}_j)}{N-1}}$$\n",
    "\n",
    "---\n",
    "> <strong> IMPORTANTE: </strong> Puesto que los elementos de valor -999.0 son elementos que carecen de importancia o que no están disponibles para el evento en la muestra de datos, no se deben de incluir en la normalización de la muestra. Por lo tanto, se excluyen del calculo de la media y desviación estándar de los valores de cada variable. En lugar de aplicarles la normalización como al resto de variables, el valor de -999.0 se sustituye por otro más cercano al valor 0, como -5.0. De esta manera, en la muestra normalizada, todos estos elementos tienen el valor -5.0, y la red neuronal tendrá que aprender que ese valor carece de significado.\n",
    "\n",
    "---\n",
    "\n",
    "Los vectores del entrenamiento se van a construir directamente con los valores normalizados, para ahorrar tiempo. Los valores medios de cada variable y sus desviaciones estándar se calculan directamente de las columnas del objeto DataFrame excluyendo los valores -999.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization:\n",
    "means = {} # dict with the means of every feature\n",
    "stds = {} # dict with the standard deviations of every feature\n",
    "\n",
    "for feature in features:\n",
    "    true_values = data.loc[data[feature] != -999.0, feature]\n",
    "    means[feature] = true_values.values.mean()\n",
    "    stds[feature] = true_values.values.std()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 La estructura de las muestras\n",
    "\n",
    "En este apartado se describe como es <strong>la implementación en el código para generar la matriz que va a entrenar la red</strong>. Para ahorrar tiempo (la estructura de vectores es grande) los elementos de la matriz se normalizan directamente.\n",
    "\n",
    "La idea es generar dos matrices, una con las muestras de señal y otra con las muestras de fondo. De esta manera se podrá controlar con más facilidad que muestras se quieren utilizar para entrenar la red y cuales se quieren reservar para evaluar su rendimiento después. Por lo tanto, se va a tener una matriz con los eventos de señal de dimension $N_{\\text{señal}}$ x 30 y otra con los eventos de fondo de dimension $N_{\\text{fondo}}$ x 30.\n",
    "\n",
    "#### IMPLEMENTACIÓN:\n",
    "\n",
    "El primer paso es crear una matriz vacía definiendo el número de filas y el número de columnas:\n",
    "```\n",
    "matrix = np.zeros(shape = (n_filas, n_columnas))\n",
    "```\n",
    "\n",
    "Después hay que identificar los índices de las filas del objeto ```pandas.DataFrame``` llamado ```data``` con los eventos (señal o fondo) que se van a utilizar para llenar la matriz (el número de índices debe corresponderse con el número de filas de la matriz que se va a llenar):\n",
    "\n",
    "```\n",
    "indices = data.loc[data['Label'] == 'Label'].index.values\n",
    "```\n",
    "\n",
    "Una vez identificados se recorren esas filas del ```pandas.DataFrame``` (eventos seleccionados) y cada una de las variables con un bucle ```for``` doble. Se accede al valor:\n",
    "\n",
    "```\n",
    "valor = data.loc[evento][variable]\n",
    "```\n",
    "\n",
    "y si ese valor es distinto de -999.0 entonces se normaliza con los valores de la media y desviación estándar definidos previamente y se llena el elemento de la matriz:\n",
    "\n",
    "```\n",
    "matriz[fila][variable] = (valor - medias[variable])/stds[variable]\n",
    "```\n",
    "\n",
    "si se quiere se puede redefinir otro valor para el valor nulo, siempre y cuando sea siempre el mismo.\n",
    "\n",
    "---\n",
    "> <strong>Nota:</strong> Este paso recorre las 30 variables de las 250000 muestras, por lo que tarda cierto tiempo. Con la implementación de este código crear la estructura de vectores separados en clases lleva 1 hora y 15 minutos, aproximadamente.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # to know how much time does the array creation takes\n",
    "\n",
    "x_signal = np.zeros(shape = (n_signal, 30)) # empty signal array\n",
    "x_background = np.zeros(shape = (n_background, 30)) # empty background array\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "### Signal array construction\n",
    "\n",
    "signal_idx = data.loc[data['Label'] == 's'].index.values\n",
    "signal_progress = [int(i*float(len(signal_idx))) for i in np.linspace(0, 1, 101)] # list for printing progress\n",
    "\n",
    "# Loop over the signal events:\n",
    "for i,s in enumerate(signal_idx):\n",
    "    for j,feature in enumerate(features):\n",
    "        \n",
    "        value = data.loc[s][feature]\n",
    "        if (value != -999.0): x_signal[i][j] = (value - means[feature])/stds[feature]\n",
    "        else: x_signal[i][j] = -5.0\n",
    "            \n",
    "    # Print progress:\n",
    "    if (i in signal_progress):\n",
    "        print(\"Signal: \"+ str(i)+\"/\"+str(len(signal_idx)) +\" completed\")\n",
    "            \n",
    "### Background array construction\n",
    "\n",
    "background_idx = data.loc[data['Label'] == 'b'].index.values\n",
    "background_progress = [int(i*float(len(background_idx))) for i in np.linspace(0, 1, 101)] # list for printing progress\n",
    "\n",
    "# Loop over the background events:\n",
    "for i,s in enumerate(background_idx):\n",
    "    for j,feature in enumerate(features):\n",
    "        \n",
    "        value = data.loc[s][feature]\n",
    "        if (value != -999.0): x_background[i][j] = (value - means[feature])/stds[feature]\n",
    "        else: x_background[i][j] = -5.0\n",
    "            \n",
    "    # Print progress:\n",
    "    if (i in background_progress):\n",
    "        print(\"Background: \"+ str(i)+\"/\"+str(len(background_idx)) +\" completed\")\n",
    "        \n",
    "stop = time.time()\n",
    "print(\"\\n\")\n",
    "print(\">>> Used time: \" + str(stop - start))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4  Definición de los sets de entrenamiento: train set y test set\n",
    "\n",
    "<p> Con los numpy arrays construidos en el paso anterior se quieren construir los numpy arrays del entrenamiento: </p>\n",
    "<ul>\n",
    "    <li> Un numpy array <strong>x_train</strong> con los numpy arrays que van a entrenar la red </li> \n",
    "    <li> Un numpy array <strong>y_train</strong> con un array para cada array de <strong>x_train</strong> que indice la clase: <em>np.array([0])</em> si es de fondo o <em>np.array([1])</em> si es de señal. </li>\n",
    "</ul>\n",
    "\n",
    "<p> Y también se tienen que reservar unos eventos para evaluar el performance de la red una vez entrenada: </p>\n",
    "<ul>\n",
    "    <li> Un numpy array <strong>x_test</strong> con los numpy arrays que van a ser clasificados por la red </li> \n",
    "    <li> Un numpy array <strong>y_test</strong> con un array para cada array de <strong>x_test</strong> que indice la clase: <em>np.array([0])</em> si es de fondo o <em>np.array([1])</em> si es de señal. </li>\n",
    "</ul>\n",
    "\n",
    "Primero se deciden los eventos <strong>de cada clase</strong> que van a componer los arrays finales <strong>x_train</strong> y <strong>x_test</strong>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_signal)\n",
    "\n",
    "n_test = 5000 # events of each class reserved for test\n",
    "\n",
    "# train samples:\n",
    "x_train_signal = x_signal[n_test:]\n",
    "x_train_background = x_background[n_test:]\n",
    "\n",
    "# test samples:\n",
    "x_test_signal = x_signal[:n_test]\n",
    "x_test_background = x_background[:n_test]\n",
    "\n",
    "print(\"Number of signal events in training set: \" + str(len(x_train_signal)))\n",
    "print(\"Number of background events in training set: \" + str(len(x_train_background)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Balance de clases\n",
    "\n",
    "<p>En este punto, se debe de tratar el problema del <strong>balance de las clases</strong>. Los dos sets de muestras, train y test set, tienen que estar equilibrados en cuanto al número de muestras de cada clase que contienen. Esto se hace para evitar dos problemas en la clasificación:</p>\n",
    "<ul> \n",
    "    <li> Si el train set tiene muchas más muestras de una clase que de otra es muy posible que \"aprenda\" a clasificar mejor una de las dos clases. Por ejemplo, suponiendo que tenemos muchos eventos de fondo y pocos de señal. La red sabrá decir que un evento es de fondo, pero al leer un evento de señal no sabrá distinguirlo y probablemente lo clasifique como fondo también. Lo más probable es que acabe clasificando todas las muestras como fondo, independientemente de la forma que tengan.</li>\n",
    "    <li> Si el test set tiene diferente número de muestras de una clase que de otra entonces nuestra manera de evaluar la red no está bien planteada. Si por ejemplo la red clasifica mejor las muestras de una clase que las de otra y además hay muchas más muestras de esa clase en el test set, nos dará un muy alto nivel de acierto sin ser verdad, o uno muy malo al contrario.</li>\n",
    "</ul>\n",
    "\n",
    "El balance de clases en el test set se resuelve inmediatamente sacando el mismo número de muestras de una clase y de otra de las muestras totales.\n",
    "\n",
    "<p>En cambio las muestras que quedan en el train set pueden no estar balanceadas, como es el caso. Esto se puede abordar de varias maneras:</p>\n",
    "<ul>\n",
    "    <li> Pesando las clases a nivel de entrenamiento. Consiste en hacer que la red de más importancia al entrenamiento de una clase que a otra añadiendo un mayor coste de esa clase en la función de perdida.</li>\n",
    "    <li> Pesando las muestras a nivel de entrenamiento. Exactamente igual que el pesado de clases pero muestra a muestra </li>\n",
    "    <li> Duplicar algunas muestras del train set (de la clases menos abundante) hasta igualar el número, para reducir el efecto. </li>\n",
    "</ul>\n",
    "\n",
    "En este ejemplo, se recurre a la última opción y se clonan algunas muestras de señal para igualar al número de muestras de fondo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_extra = np.zeros(shape = (n_background-n_signal, 30)) # empty array that will contain the clonned samples\n",
    "\n",
    "sample_index = 0 # index of the sample that is clonned\n",
    "\n",
    "for i in range(0, len(x_extra)):\n",
    "    \n",
    "    x_extra[i] = x_train_signal[sample_index]    \n",
    "    sample_index += 1\n",
    "    \n",
    "    if (sample_index > len(x_train_signal) -1): sample_index = 0\n",
    "        \n",
    "# Add the clonned samples to the signal train set:        \n",
    "x_train_signal = np.concatenate((x_train_signal, x_extra), axis = 0)\n",
    "\n",
    "# Check if the classes are balanced know:\n",
    "print(\"Number of signal events in training set: \" + str(len(x_train_signal)))\n",
    "print(\"Number of background events in training set: \" + str(len(x_train_background)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Construcción final de los train y test sets\n",
    "\n",
    "Una vez las muestras estan balanceadas, se crean los arrays de las muestras, <strong>x_train</strong> y <strong>x_test</strong>, y de las etiquetas, <strong>y_train</strong> y <strong>y_test</strong>.\n",
    "\n",
    "Una vez creados, los las muestras se mezclan de manera aleatoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "### Creation of the label arrays:\n",
    "y_train_signal = np.full(shape = (len(x_train_signal), 1), fill_value = 1)\n",
    "y_test_signal = np.full(shape = (len(x_test_signal), 1), fill_value = 1)\n",
    "y_train_background = np.full(shape = (len(x_train_background), 1), fill_value = 0)\n",
    "y_test_background = np.full(shape = (len(x_test_background), 1), fill_value = 0)\n",
    "\n",
    "### Creation of the final samples\n",
    "x_train = np.concatenate((x_train_signal, x_train_background), axis = 0)\n",
    "x_test = np.concatenate((x_test_signal, x_test_background), axis = 0)\n",
    "y_train = np.concatenate((y_train_signal, y_train_background), axis = 0)\n",
    "y_test = np.concatenate((y_test_signal, y_test_background), axis = 0)\n",
    "\n",
    "### Random shuffle the samples\n",
    "# Train and test indexes\n",
    "idx_train = np.arange(y_train.shape[0])\n",
    "idx_test = np.arange(y_test.shape[0])\n",
    "\n",
    "# Shuffle the indexes\n",
    "random.shuffle(idx_train)\n",
    "random.shuffle(idx_test)\n",
    "\n",
    "# Order the samples according to the new indexes distribution\n",
    "y_train = y_train[idx_train]\n",
    "x_train = x_train[idx_train]\n",
    "y_test = y_test[idx_test]\n",
    "x_test = x_test[idx_test]\n",
    "\n",
    "x_train\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Guardar las variables del entrenamiento\n",
    "\n",
    "Como el proceso de crear las muestras es largo y lleva cierto tiempo, no es aconsejable crear las muestras cada vez que la red neuronal se entrena.\n",
    "\n",
    "Los train y test sets generados se guardan en un fichero pickle que permitirá acceder a ellos más adelante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('training_variables.p', 'wb') as file_:\n",
    "        pickle.dump([x_train, y_train, x_test, y_test], file_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
